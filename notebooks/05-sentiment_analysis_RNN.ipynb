{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with an RNN using Word Embedding\n",
    "\n",
    "The following implements word embedding to learn the sentiment (positive or negative) or movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T MODIFY THIS CELL\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" \n",
    "\n",
    "# import some modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T MODIFY THIS CELL\n",
    "\n",
    "# import the data\n",
    "reviews_url = 'https://raw.githubusercontent.com/prof-groff/deep-learning/master/data/sentiment/reviews.txt'\n",
    "labels_url = 'https://raw.githubusercontent.com/prof-groff/deep-learning/master/data/sentiment/labels.txt'\n",
    "reviews = requests.get(reviews_url).text\n",
    "labels = requests.get(labels_url).text\n",
    "\n",
    "# do some additional preprocessing\n",
    "reviews = re.sub(' br ','', reviews) # remove random remnants of <br> tags\n",
    "reviews = re.sub('\\n', 'newline_char', reviews) # temporarily remove \\n characters\n",
    "reviews = re.sub('\\s+', ' ', reviews) # remove all special characters and extra spaces\n",
    "reviews = re.sub('newline_char', '\\n', reviews) # replace the \\n characters\n",
    "\n",
    "# split the reviews and lables text at \\n characters to make lists\n",
    "reviews_list = reviews.split('\\n')[0:-1] # for some reason the last element is junk\n",
    "labels_list = labels.split('\\n')[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T MODIFY THIS CELL\n",
    "\n",
    "# look at an example negative and positive review\n",
    "print(labels_list[1001].upper())\n",
    "print('\\n')\n",
    "print(reviews_list[1001])\n",
    "print('\\n')\n",
    "print(labels_list[1002].upper())\n",
    "print('\\n')\n",
    "print(reviews_list[1002])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T MODIFY THIS CELL\n",
    "\n",
    "# look at the distribution of review lengths (# of words)\n",
    "# some of the reviews are really long but most are under 500 words long\n",
    "review_lengths = []\n",
    "for each in reviews_list:\n",
    "    review_lengths.append(len(each.split()))\n",
    "\n",
    "plt.hist(review_lengths,bins=50)\n",
    "plt.ylabel('counts')\n",
    "plt.xlabel('review length')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: PICK A MAX_LENGTH (AT LEAST 100) AND TRIM THE DATA TO ONLY THOSE REVIEWS \n",
    "# WITH NO MORE THAN THIS MANY WORDS\n",
    "max_length = \n",
    "\n",
    "# DO NOT MODIFY THIS CELL BELOW THIS POINT\n",
    "\n",
    "reviews_trimmed = []\n",
    "labels_trimmed = []\n",
    "\n",
    "for r, l in zip(reviews_list, labels_list):\n",
    "    if len(r.split())<= max_length:\n",
    "        reviews_trimmed.append(r)\n",
    "        labels_trimmed.append(l)\n",
    "        \n",
    "print('TOTAL REVIEWS: ' + str(len(reviews_list)))\n",
    "print('REVIEWS WITH < ' + str(max_length) + ' WORDS: ' + str(len(reviews_trimmed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "# determine the number of unique words in the reviews\n",
    "vocab = set(' '.join(reviews_trimmed).split())\n",
    "vocab_length = len(vocab)+1 # add one so zero is reserved for padding\n",
    "print('VOCAB LENGTH: ' + str(vocab_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "# a function to create mappings between words and integers\n",
    "# each integer is a key for each word and vice versa\n",
    "def create_lookup_tables(vocab):\n",
    "    # enumerate adds an index to each word in the vocab and returns a list of tuples\n",
    "    int_to_vocab = dict(enumerate(vocab, 1))\n",
    "    vocab_to_int = dict(zip(int_to_vocab.values(), int_to_vocab.keys()))\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "# the following tokenizes the reviews and labels\n",
    "n_reviews = len(reviews_trimmed)\n",
    "reviews_vect = np.zeros([n_reviews, max_length])\n",
    "labels_vect = np.zeros([n_reviews,2])\n",
    "for ii, r, l in zip(np.arange(n_reviews), reviews_trimmed, labels_trimmed):\n",
    "    words = r.split()\n",
    "    n_words = len(words)\n",
    "    for jj, w in zip(np.arange(n_words), words):\n",
    "        reviews_vect[ii, max_length-n_words+jj] = vocab_to_int[w]\n",
    "    if l == 'positive':\n",
    "        labels_vect[ii,1]=1\n",
    "    else:\n",
    "        labels_vect[ii,0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "# look at an example tokenized review\n",
    "\n",
    "print(labels_trimmed[0].upper())\n",
    "print('\\n')\n",
    "print(reviews_trimmed[0])\n",
    "print('\\n')\n",
    "print(reviews_vect[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: BREAK THE DATA INTO A TRAINING, VALIDATION, AND TESTING SETS \n",
    "# WITH AT LEAST 1000 ELEMENTS IN THE VALIDATION AND TESTING SETS\n",
    "val_size = \n",
    "test_size = \n",
    "\n",
    "# DO NOT MODIFY THIS CELL BELOW THIS POINT\n",
    "\n",
    "x_train, x_val, x_test = reviews_vect[:-(val_size+test_size)], reviews_vect[-(val_size+test_size):-test_size], reviews_vect[-test_size:]\n",
    "y_train, y_val, y_test = labels_vect[:-(val_size+test_size)], labels_vect[-(val_size+test_size):-test_size], labels_vect[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: SELECT MODEL SIZE PARAMETERS\n",
    "embedding_size = \n",
    "memory_units = \n",
    "\n",
    "# TO-DO: SELECT HYPERPARAMETERS\n",
    "batch_size = \n",
    "epochs = \n",
    "learning_rate = \n",
    "\n",
    "# BUILD A MODEL\n",
    "print('build model...'.upper())\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length, output_dim=embedding_size, input_length=max_length))\n",
    "# TO-DO: ADD A LSTM LAYER\n",
    "model.add()\n",
    "# TO-DO: ADD A DENSE (FULLY CONNECTED) LAYER WITH 2 OUTPUT UNITS AND SOFTMAX ACTIVATION\n",
    "model.add()\n",
    "\n",
    "optimizer = RMSprop(lr=learning_rate)\n",
    "# ALTERNATIVE OPTIMIZER\n",
    "# optimizer = 'adam'\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "print('\\ntrain...\\n'.upper())\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val, y_val))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('\\ntest score: '.upper() + str(score))\n",
    "print('test accuracy: '.upper() + str(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at a specific review and compare the predicted sentiment to the actual sentiment\n",
    "\n",
    "# TO-DO: PICK A TEST CASE\n",
    "test_case =  # index of one of the reviews in the test set\n",
    "\n",
    "# DO NOT MODIFY THIS CELL BELOW THIS POINT\n",
    "\n",
    "x = x_test[test_case]\n",
    "y = y_test[test_case]\n",
    "\n",
    "# convert from tokens back to text\n",
    "test_review = []\n",
    "for each in x:\n",
    "    if each != 0:\n",
    "        test_review.append(int_to_vocab[each])\n",
    "\n",
    "test_review = ' '.join(test_review)\n",
    "\n",
    "# look at an example negative and positive review\n",
    "print('predicting sentiment of following review...\\n'.upper())\n",
    "print(test_review)\n",
    "print('\\n')\n",
    "\n",
    "print('passing the review through the trained model...\\n'.upper())\n",
    "x = np.reshape(x, (1,len(x)))\n",
    "y_prime = model.predict(x)\n",
    "print('predicted sentiment - 0 - negative, 1 - positive'.upper())\n",
    "print(y_prime[0])\n",
    "print(['negative', 'positive'][np.argmax(y_prime[0])].upper())\n",
    "print('\\n')\n",
    "print('actual sentiment'.upper())\n",
    "print(['negative', 'positive'][np.argmax(y)].upper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHALLENGE: WRITE YOUR OWN REVIEW AN USE THE MODEL TO PREDICT ITS SENTIMENT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
