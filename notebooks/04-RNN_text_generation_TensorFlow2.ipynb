{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Text Generation\n",
    "\n",
    "This notebook implements a recurrent neural network that learns to compose sonnets after being trained by Shakespeare. A character level approach is used. Hidden layers use LSTM units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare's Sonnets or Frankenstein\n",
    "\n",
    "A previously processed text file containing all of Shakespeare's sonnets or the text of Frankenstein by Mary Shelley is imported. The character vocabulary and dictionaries to make characters to indexes and vice versa are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT LENGTH: 94687\n",
      "\n",
      "TEXT SAMPLE:\n",
      "\n",
      "i\n",
      "from fairest creatures we desire increase,\n",
      "that thereby beauty's rose might never die,\n",
      "but as the riper should by time decease,\n",
      "his tender heir might bear his memory:\n",
      "but thou, contracted to thine own bright eyes,\n",
      "feed'st thy light's flame with self-substantial fuel,\n",
      "making a famine where abundance lies,\n",
      "thy self thy foe, to thy sweet self too cruel:\n",
      "thou that art now the world's fresh ornament,\n",
      "and only herald to the gaudy spring,\n",
      "within thine own bud buriest thy content,\n",
      "and tender churl mak'st waste in niggarding:\n",
      "pity the world, or else this glutton be,\n",
      "to eat the world's due, by the grave and thee.\n"
     ]
    }
   ],
   "source": [
    "# read in preprocessed text of shakespeare's sonnets \n",
    "\n",
    "# from local file system\n",
    "# filename = 'sonnets.txt'\n",
    "# filename = 'frankenstein.txt'\n",
    "# file = open(filename,'r')\n",
    "# text = file.read()\n",
    "\n",
    "# from github\n",
    "url = 'https://raw.githubusercontent.com/prof-groff/deep-learning/master/data/sonnets.txt'\n",
    "# url = 'https://raw.githubusercontent.com/prof-groff/deep-learning/master/data/frankenstein.txt'\n",
    "text = requests.get(url).text\n",
    "\n",
    "len_text = len(text)\n",
    "print('text length: '.upper() + str(len(text)) + '\\n')\n",
    "print('text sample:'.upper() + '\\n')\n",
    "print(text[0:612]) # show some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 38\n"
     ]
    }
   ],
   "source": [
    "# created some dictionaries\n",
    "chars = sorted(list(set(text)))\n",
    "len_chars = len(chars)\n",
    "print('total chars: ' + str(len_chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is chunked up into sequences of uniform length. Each sequence is a feature and has a target corresponding to the character in the text immediately following the sequence. The features and targets are then vectorized. Each character is converted to a Boolean vectors having only one true element at the possiton corresponding to the character. <em>Note: making these vectorized features and targets integers (0s and 1s) instead of booleans seriously hampers learning and I am not sure why.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF FEATURES (SEQUENCES):31520\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "seq_length = 128\n",
    "step = 3\n",
    "features = []\n",
    "targets = []\n",
    "for i in range(0, len_text - seq_length, step):\n",
    "    features.append(text[i: i + seq_length])\n",
    "    targets.append(text[i + seq_length])\n",
    "num_features = len(features)\n",
    "print('number of features (sequences):'.upper() + str(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((num_features, seq_length, len_chars), dtype=np.bool)\n",
    "y = np.zeros((num_features, len_chars), dtype=np.bool)\n",
    "for i, feature in enumerate(features):\n",
    "    for t, char in enumerate(feature):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[targets[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model Graphs\n",
    "\n",
    "Two RNN graphs are implemented here. One is more deep and less wide and the other is less deep and more wide. The model parameters have been tuned to give decent results. A function is also defined to allow sampling of the train modeled in order to generate new character sequences. In additoin, a function is defined which is called at the end of each epoch to generate and display generated character sequences as the network learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model(seq_length, n_chars):\n",
    "    print('buildinng two-layer model with 128 memory units...'.upper()+'\\n')\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(seq_length, n_chars)))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "    optimizer = RMSprop(lr=0.005)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def wide_model(seq_length, n_chars):\n",
    "    print('building one-layer model with 256 memory units...'.upper()+'\\n')\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(seq_length, n_chars)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "    optimizer = RMSprop(lr=0.005)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def sample(probs, method=0):\n",
    "    probs = np.asarray(probs).astype('float64')\n",
    "    probs = probs/np.sum(probs)\n",
    "    \n",
    "    # helper function to sample an index from a probability array\n",
    "    if method == 0: \n",
    "        # method 0: just return index of character with the highest probability\n",
    "        index = np.argmax(probs)\n",
    "    elif method == 1: \n",
    "        # method 1: draw a random number from 0 to 1, calculate the cumulative sum of the prediction vector.\n",
    "        # return the index of the first element in the cumulative sum greater than the random number\n",
    "        index = np.argwhere(np.cumsum(probs)>np.random.uniform())[0][0]\n",
    "    elif method == 2:\n",
    "        # method 2: draw an element from a multinomial distribution defined by the given probabilities\n",
    "        index = np.argmax(np.random.multinomial(1, probs, 1))\n",
    "    elif method == 3:\n",
    "        # method 3: emphasis larger probabilities and diminish smaller probabilities by doing a log transform\n",
    "        temperature = 0.5 # less than one increases differences between small and large probabilities\n",
    "        probs = np.log(probs) / temperature # same as method 2 with temperature = 1\n",
    "        probs = np.exp(probs) # undo log transform\n",
    "        probs = probs / np.sum(probs)\n",
    "        index = np.argmax(np.random.multinomial(1, probs, 1))\n",
    "        \n",
    "    return index\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    if (epoch)%10 == 0:\n",
    "\n",
    "        print('\\nGENERATING CHARACTER SEQUENCE AFTER EPOCH: {}\\n'.format(epoch+1))\n",
    "\n",
    "        start_index = random.randint(0, len_text - seq_length - 1)\n",
    "       \n",
    "        \n",
    "        seed = text[start_index: start_index + seq_length]\n",
    "        print('GENERATING WITH SEED:\\n\\n' + seed + '\\n')\n",
    "        \n",
    "        \n",
    "        print('MAX SAMPLING:\\n')\n",
    "        phrase = seed\n",
    "        generated = ''\n",
    "        generated += phrase\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(seq_length):\n",
    "            x_pred = np.zeros((1, seq_length, len_chars))\n",
    "            for t, char in enumerate(phrase):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            probs = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(probs, method=0)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            phrase = phrase[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print('\\n')\n",
    "        \n",
    "        print('PROBABILISTIC SAMPLING:\\n')\n",
    "        phrase = seed\n",
    "        generated = ''\n",
    "        generated += phrase\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(seq_length):\n",
    "            x_pred = np.zeros((1, seq_length, len_chars))\n",
    "            for t, char in enumerate(phrase):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            probs = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(probs, method=1)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            phrase = phrase[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDINNG TWO-LAYER MODEL WITH 128 MEMORY UNITS...\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128, 128)          85504     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 38)                4902      \n",
      "=================================================================\n",
      "Total params: 221,990\n",
      "Trainable params: 221,990\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Train on 31520 samples\n",
      "Epoch 1/41\n",
      "31488/31520 [============================>.] - ETA: 0s - loss: 2.8285 - accuracy: 0.2124\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 1\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "t hath my duty strongly knit,\n",
      "to thee i send this written embassage,\n",
      "to witness duty, not to show my wit:\n",
      "duty so great, which w\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "t hath my duty strongly knit,\n",
      "to thee i send this written embassage,\n",
      "to witness duty, not to show my wit:\n",
      "duty so great, which we t te t te to t te t te to t te t te to t te t te to t te t te to t te t te to t te t te to t te t te to t te t te to t te t te\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "t hath my duty strongly knit,\n",
      "to thee i send this written embassage,\n",
      "to witness duty, not to show my wit:\n",
      "duty so great, which we loce teen sl tt ge:\n",
      "cer w, brs  f or tegr tm ,pang og th  ot st r lld,\n",
      "f t d brart th, touen, fe eed pp;esh sasd ar es,\n",
      "t se? \n",
      "\n",
      "31520/31520 [==============================] - 22s 690us/sample - loss: 2.8281 - accuracy: 0.2125\n",
      "Epoch 2/41\n",
      "31520/31520 [==============================] - 6s 203us/sample - loss: 2.2499 - accuracy: 0.3435\n",
      "Epoch 3/41\n",
      "31520/31520 [==============================] - 6s 197us/sample - loss: 2.0247 - accuracy: 0.3979\n",
      "Epoch 4/41\n",
      "31520/31520 [==============================] - 6s 197us/sample - loss: 1.8921 - accuracy: 0.4319\n",
      "Epoch 5/41\n",
      "31520/31520 [==============================] - 6s 197us/sample - loss: 1.7888 - accuracy: 0.4580\n",
      "Epoch 6/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 1.7015 - accuracy: 0.4804\n",
      "Epoch 7/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 1.6277 - accuracy: 0.4970\n",
      "Epoch 8/41\n",
      "31520/31520 [==============================] - 6s 197us/sample - loss: 1.5525 - accuracy: 0.5203\n",
      "Epoch 9/41\n",
      "31520/31520 [==============================] - 6s 197us/sample - loss: 1.4776 - accuracy: 0.5384\n",
      "Epoch 10/41\n",
      "31520/31520 [==============================] - 6s 197us/sample - loss: 1.3987 - accuracy: 0.5585\n",
      "Epoch 11/41\n",
      "31488/31520 [============================>.] - ETA: 0s - loss: 1.3221 - accuracy: 0.5819\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 11\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "w,\n",
      "with time's injurious hand crush'd and o'erworn;\n",
      "when hours have drain'd his blood and fill'd his brow\n",
      "with lines and wrinkle\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "w,\n",
      "with time's injurious hand crush'd and o'erworn;\n",
      "when hours have drain'd his blood and fill'd his brow\n",
      "with lines and wrinkles of the wound,\n",
      "the panse i am me tome so beseng:\n",
      "the panse i am me tome so beseng:\n",
      "the pansing me to be that be thou stay see,\n",
      "\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "w,\n",
      "with time's injurious hand crush'd and o'erworn;\n",
      "when hours have drain'd his blood and fill'd his brow\n",
      "with lines and wrinkles to eng, to the woess,\n",
      "and lauds age karso.\n",
      "axting of gets to defaqe,\n",
      "sweet dostand as now muse,\n",
      "and what as yous, and be offor\n",
      "\n",
      "31520/31520 [==============================] - 16s 520us/sample - loss: 1.3221 - accuracy: 0.5819\n",
      "Epoch 12/41\n",
      "31520/31520 [==============================] - 6s 204us/sample - loss: 1.2455 - accuracy: 0.6049\n",
      "Epoch 13/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 1.1703 - accuracy: 0.6251\n",
      "Epoch 14/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 1.0938 - accuracy: 0.6503\n",
      "Epoch 15/41\n",
      "31520/31520 [==============================] - 6s 197us/sample - loss: 1.0234 - accuracy: 0.6732\n",
      "Epoch 16/41\n",
      "31520/31520 [==============================] - 6s 197us/sample - loss: 0.9521 - accuracy: 0.6916\n",
      "Epoch 17/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.8895 - accuracy: 0.7124\n",
      "Epoch 18/41\n",
      "31520/31520 [==============================] - 6s 197us/sample - loss: 0.8243 - accuracy: 0.7343\n",
      "Epoch 19/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.7655 - accuracy: 0.7531\n",
      "Epoch 20/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.7176 - accuracy: 0.7666\n",
      "Epoch 21/41\n",
      "31488/31520 [============================>.] - ETA: 0s - loss: 0.6681 - accuracy: 0.7827\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 21\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "why then her breasts are dun;\n",
      "if hairs be wires, black wires grow on her head.\n",
      "i have seen roses damask'd, red and white,\n",
      "but no\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "why then her breasts are dun;\n",
      "if hairs be wires, black wires grow on her head.\n",
      "i have seen roses damask'd, red and white,\n",
      "but not the beauty love art the woes,\n",
      "nor doth prime thy self the world call hear,\n",
      "whilst no surchase these marresself the world,\n",
      "thes\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "why then her breasts are dun;\n",
      "if hairs be wires, black wires grow on her head.\n",
      "i have seen roses damask'd, red and white,\n",
      "but not thy good gone, not this proves, eich borthmesce shall.\n",
      "cxlvi\n",
      "is of floweds encerive alimen intiin ow shall from beartrest,\n",
      "i a\n",
      "\n",
      "31520/31520 [==============================] - 16s 511us/sample - loss: 0.6684 - accuracy: 0.7826\n",
      "Epoch 22/41\n",
      "31520/31520 [==============================] - 6s 205us/sample - loss: 0.6320 - accuracy: 0.7925\n",
      "Epoch 23/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.5911 - accuracy: 0.8084\n",
      "Epoch 24/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.5616 - accuracy: 0.8163\n",
      "Epoch 25/41\n",
      "31520/31520 [==============================] - 6s 197us/sample - loss: 0.5319 - accuracy: 0.8264\n",
      "Epoch 26/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.4975 - accuracy: 0.8358\n",
      "Epoch 27/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.4736 - accuracy: 0.8460\n",
      "Epoch 28/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.4615 - accuracy: 0.8466\n",
      "Epoch 29/41\n",
      "31520/31520 [==============================] - 6s 197us/sample - loss: 0.4374 - accuracy: 0.8546\n",
      "Epoch 30/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.4195 - accuracy: 0.8622\n",
      "Epoch 31/41\n",
      "31488/31520 [============================>.] - ETA: 0s - loss: 0.4064 - accuracy: 0.8646\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 31\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "\n",
      "how with this rage shall beauty hold a plea,\n",
      "whose action is no stronger than a flower?\n",
      "o! how shall summer's honey breath hold\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "\n",
      "how with this rage shall beauty hold a plea,\n",
      "whose action is no stronger than a flower?\n",
      "o! how shall summer's honey breath hold presine,\n",
      "the barts beauty be chal seem and hearths can bet,\n",
      "love my love the flour reauty his aby drow;\n",
      "for i have tweed his fo\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "\n",
      "how with this rage shall beauty hold a plea,\n",
      "whose action is no stronger than a flower?\n",
      "o! how shall summer's honey breath hold sice in tered,\n",
      "as in my muet most no fair sweet metty the story;\n",
      "that i have hat love he tullt gaits is love,\n",
      "sall chinco beaut\n",
      "\n",
      "31520/31520 [==============================] - 16s 517us/sample - loss: 0.4064 - accuracy: 0.8645\n",
      "Epoch 32/41\n",
      "31520/31520 [==============================] - 6s 204us/sample - loss: 0.3970 - accuracy: 0.8676\n",
      "Epoch 33/41\n",
      "31520/31520 [==============================] - 6s 197us/sample - loss: 0.3856 - accuracy: 0.8707\n",
      "Epoch 34/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.3671 - accuracy: 0.8782\n",
      "Epoch 35/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.3647 - accuracy: 0.8791\n",
      "Epoch 36/41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.3504 - accuracy: 0.8819\n",
      "Epoch 37/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.3449 - accuracy: 0.8843\n",
      "Epoch 38/41\n",
      "31520/31520 [==============================] - 6s 197us/sample - loss: 0.3411 - accuracy: 0.8848\n",
      "Epoch 39/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.3240 - accuracy: 0.8910\n",
      "Epoch 40/41\n",
      "31520/31520 [==============================] - 6s 196us/sample - loss: 0.3224 - accuracy: 0.8935\n",
      "Epoch 41/41\n",
      "31488/31520 [============================>.] - ETA: 0s - loss: 0.3239 - accuracy: 0.8914\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 41\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "es have thorns, and silver fountains mud:\n",
      "clouds and eclipses stain both moon and sun,\n",
      "and loathsome canker lives in sweetest bu\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "es have thorns, and silver fountains mud:\n",
      "clouds and eclipses stain both moon and sun,\n",
      "and loathsome canker lives in sweetest budded.\n",
      "the raintance to be, rettered my verse is love,\n",
      "to time thy fairet made still in outtifor was,\n",
      "i is the dare weeth forther\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "es have thorns, and silver fountains mud:\n",
      "clouds and eclipses stain both moon and sun,\n",
      "and loathsome canker lives in sweetest budne,\n",
      "intermed it the fingers, my loves thou my lovely arpound,\n",
      "which i stold contwern deel me hape to frow fall,\n",
      "genenger apliet\n",
      "\n",
      "31520/31520 [==============================] - 16s 519us/sample - loss: 0.3241 - accuracy: 0.8913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffac041a6a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deeper, less wide, model\n",
    "model = deep_model(seq_length, len_chars)\n",
    "model.summary()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "print('\\n')\n",
    "model.fit(x, y,\n",
    "          batch_size=256,\n",
    "          epochs=41,\n",
    "          # epochs=21, # for frankenstein use fewer epochs because the text is much longer\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING ONE-LAYER MODEL WITH 256 MEMORY UNITS...\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 256)               302080    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 38)                9766      \n",
      "=================================================================\n",
      "Total params: 311,846\n",
      "Trainable params: 311,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "Epoch 1/41\n",
      "31520/31520 [==============================] - 13s 405us/step - loss: 2.8310 - acc: 0.2103\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 1\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "eir eyes were kind,\n",
      "to thy fair flower add the rank smell of weeds:\n",
      "but why thy odour matcheth not thy show,\n",
      "the soil is this, t\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "eir eyes were kind,\n",
      "to thy fair flower add the rank smell of weeds:\n",
      "but why thy odour matcheth not thy show,\n",
      "the soil is this, thaue thaue thaue thaue thaue thaue thaue thaue thaue thaue thaue thaue thaue thaue thaue thaue thaue thaue thaue thaue thaue tha\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "eir eyes were kind,\n",
      "to thy fair flower add the rank smell of weeds:\n",
      "but why thy odour matcheth not thy show,\n",
      "the soil is this, thatedeieivmite.ist ii thian peeae liuae,\n",
      "uis antouaxugausuits\n",
      "eee eea,y:en iriseab.\n",
      "uutasxluge, ru ly heaur lariwcis dis inistye\n",
      "\n",
      "Epoch 2/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 2.2895 - acc: 0.3397\n",
      "Epoch 3/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 2.0898 - acc: 0.3833\n",
      "Epoch 4/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 1.9684 - acc: 0.4112\n",
      "Epoch 5/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 1.8771 - acc: 0.4346\n",
      "Epoch 6/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 1.7934 - acc: 0.4555\n",
      "Epoch 7/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 1.7201 - acc: 0.4732\n",
      "Epoch 8/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 1.6544 - acc: 0.4927\n",
      "Epoch 9/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 1.5918 - acc: 0.5094\n",
      "Epoch 10/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 1.5221 - acc: 0.5268\n",
      "Epoch 11/41\n",
      "31520/31520 [==============================] - 12s 390us/step - loss: 1.4549 - acc: 0.5435\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 11\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "e long-liv'd phoenix, in her blood;\n",
      "make glad and sorry seasons as thou fleets,\n",
      "and do whate'er thou wilt, swift-footed time,\n",
      "to\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "e long-liv'd phoenix, in her blood;\n",
      "make glad and sorry seasons as thou fleets,\n",
      "and do whate'er thou wilt, swift-footed time,\n",
      "to thee in my muse my self and my must my might,\n",
      "and thee spented dear fire, the will me mest my must my miget,\n",
      "so thine eye i suc\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "e long-liv'd phoenix, in her blood;\n",
      "make glad and sorry seasons as thou fleets,\n",
      "and do whate'er thou wilt, swift-footed time,\n",
      "to faury and my rase, live; weras placiee edeigm\n",
      "ok love of thou, unoos t, and myself,\n",
      "of thou usheat sie i kingle sidge of rummen\n",
      "\n",
      "Epoch 12/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 1.3852 - acc: 0.5621\n",
      "Epoch 13/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 1.3247 - acc: 0.5808\n",
      "Epoch 14/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 1.2615 - acc: 0.6013\n",
      "Epoch 15/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 1.1889 - acc: 0.6182\n",
      "Epoch 16/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 1.1234 - acc: 0.6419\n",
      "Epoch 17/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 1.0970 - acc: 0.6552\n",
      "Epoch 18/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 1.0208 - acc: 0.6779\n",
      "Epoch 19/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 0.9601 - acc: 0.6918\n",
      "Epoch 20/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 1.1068 - acc: 0.6977\n",
      "Epoch 21/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 0.8748 - acc: 0.7172\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 21\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "hee lie!\n",
      "thou art the grave where buried love doth live,\n",
      "hung with the trophies of my lovers gone,\n",
      "who all their parts of me to \n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "hee lie!\n",
      "thou art the grave where buried love doth live,\n",
      "hung with the trophies of my lovers gone,\n",
      "who all their parts of me to me, my lovely antill with my memper'd me.\n",
      "xxxxvii\n",
      "whe will will i what is the bestring make st me,\n",
      "make gall what is my jecture \n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "hee lie!\n",
      "thou art the grave where buried love doth live,\n",
      "hung with the trophies of my lovers gone,\n",
      "who all their parts of me to trourle sell belie,\n",
      "bay my must e child and meaking of the flound,\n",
      "than thou ants of love'd drad me that they?\n",
      "if i love sweetse\n",
      "\n",
      "Epoch 22/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 0.8375 - acc: 0.7291\n",
      "Epoch 23/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 0.8030 - acc: 0.7379\n",
      "Epoch 24/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 0.7709 - acc: 0.7461\n",
      "Epoch 25/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 0.7456 - acc: 0.7558\n",
      "Epoch 26/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 0.7180 - acc: 0.7649\n",
      "Epoch 27/41\n",
      "31520/31520 [==============================] - 12s 387us/step - loss: 0.6891 - acc: 0.7735\n",
      "Epoch 28/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 0.6723 - acc: 0.7787\n",
      "Epoch 29/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 0.6500 - acc: 0.7865\n",
      "Epoch 30/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 0.6322 - acc: 0.7912\n",
      "Epoch 31/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 0.6198 - acc: 0.7978\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 31\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "ove in love's fresh case,\n",
      "weighs not the dust and injury of age,\n",
      "nor gives to necessary wrinkles place,\n",
      "but makes antiquity for \n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "ove in love's fresh case,\n",
      "weighs not the dust and injury of age,\n",
      "nor gives to necessary wrinkles place,\n",
      "but makes antiquity for a then benowell,\n",
      "and beauty hou, not this placke my brease,\n",
      "when i should gease sam slow?\n",
      "it it the world beauty see,\n",
      "that mysel\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "ove in love's fresh case,\n",
      "weighs not the dust and injury of age,\n",
      "nor gives to necessary wrinkles place,\n",
      "but makes antiquity for a ther every woe.\n",
      "lxxxii\n",
      "shay my lifs within, like on inowience bousless'still'd,\n",
      "like a face, beauty good and heartleds sake,\n",
      "a\n",
      "\n",
      "Epoch 32/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 0.6078 - acc: 0.7979\n",
      "Epoch 33/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 0.5913 - acc: 0.8044\n",
      "Epoch 34/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 0.5722 - acc: 0.8118\n",
      "Epoch 35/41\n",
      "31520/31520 [==============================] - 12s 391us/step - loss: 0.5594 - acc: 0.8137\n",
      "Epoch 36/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 0.5616 - acc: 0.8128- ETA: 2s - l\n",
      "Epoch 37/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 0.5340 - acc: 0.8210\n",
      "Epoch 38/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 0.5352 - acc: 0.8192\n",
      "Epoch 39/41\n",
      "31520/31520 [==============================] - 12s 388us/step - loss: 0.5242 - acc: 0.8253\n",
      "Epoch 40/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 0.5196 - acc: 0.8290\n",
      "Epoch 41/41\n",
      "31520/31520 [==============================] - 12s 389us/step - loss: 0.5120 - acc: 0.8283\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 41\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "ne\n",
      "may make seem bare, in wanting words to show it,\n",
      "but that i hope some good conceit of thine\n",
      "in thy soul's thought, all naked,\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "ne\n",
      "may make seem bare, in wanting words to show it,\n",
      "but that i hope some good conceit of thine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in thy soul's thought, all naked, whils, and kindy,\n",
      "when thou still in this false in my cheross'd;\n",
      "and ther faret first masure of hours love,\n",
      "to the pligut the f\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "ne\n",
      "may make seem bare, in wanting words to show it,\n",
      "but that i hope some good conceit of thine\n",
      "in thy soul's thought, all naked, thengh right'd was duemy't,\n",
      "to my shame dith art, who, cruch and coones,\n",
      "thou art, and roose shoul line,\n",
      "they in this sid, and \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6adc7a850>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# less deep but wider model\n",
    "# deeper, less wide, model\n",
    "model = wide_model(seq_length, len_chars)\n",
    "model.summary()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "print('\\n')\n",
    "model.fit(x, y,\n",
    "          batch_size=256,\n",
    "          epochs=41,\n",
    "          # epochs=21, # for frankenstein use fewer epochs because the text is much longer\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
