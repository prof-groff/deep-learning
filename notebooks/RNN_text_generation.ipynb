{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Text Generation\n",
    "\n",
    "This notebook implements a recurrent neural network that learns to compose sonnets after being trained by Shakespeare. A character level approach is used. Hidden layers use LSTM units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare's Sonnets or Frankenstein\n",
    "\n",
    "A previously processed text file containing all of Shakespeare's sonnets or the text of Frankenstein by Mary Shelley is imported. The character vocabulary and dictionaries to make characters to indexes and vice versa are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT LENGTH: 94687\n",
      "\n",
      "TEXT SAMPLE:\n",
      "\n",
      "i\n",
      "from fairest creatures we desire increase,\n",
      "that thereby beauty's rose might never die,\n",
      "but as the riper should by time decease,\n",
      "his tender heir might bear his memory:\n",
      "but thou, contracted to thine own bright eyes,\n",
      "feed'st thy light's flame with self-substantial fuel,\n",
      "making a famine where abundance lies,\n",
      "thy self thy foe, to thy sweet self too cruel:\n",
      "thou that art now the world's fresh ornament,\n",
      "and only herald to the gaudy spring,\n",
      "within thine own bud buriest thy content,\n",
      "and tender churl mak'st waste in niggarding:\n",
      "pity the world, or else this glutton be,\n",
      "to eat the world's due, by the grave and thee.\n"
     ]
    }
   ],
   "source": [
    "# read in preprocessed text of shakespeare's sonnets \n",
    "\n",
    "# from local file system\n",
    "# filename = 'sonnets.txt'\n",
    "# filename = 'frankenstein.txt'\n",
    "# file = open(filename,'r')\n",
    "# text = file.read()\n",
    "\n",
    "# from github\n",
    "url = 'https://raw.githubusercontent.com/prof-groff/deep-learning/master/data/sonnets.txt'\n",
    "# url = 'https://raw.githubusercontent.com/prof-groff/deep-learning/master/data/frankenstein.txt'\n",
    "text = requests.get(url).text\n",
    "\n",
    "len_text = len(text)\n",
    "print('text length: '.upper() + str(len(text)) + '\\n')\n",
    "print('text sample:'.upper() + '\\n')\n",
    "print(text[0:612]) # show some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 38\n"
     ]
    }
   ],
   "source": [
    "# created some dictionaries\n",
    "chars = sorted(list(set(text)))\n",
    "len_chars = len(chars)\n",
    "print('total chars: ' + str(len_chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is chunked up into sequences of uniform length. Each sequence is a feature and has a target corresponding to the character in the text immediately following the sequence. The features and targets are then vectorized. Each character is converted to a Boolean vectors having only one true element at the possiton corresponding to the character. <em>Note: making these vectorized features and targets integers (0s and 1s) instead of booleans seriously hampers learning and I am not sure why.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF FEATURES (SEQUENCES):31520\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "seq_length = 128\n",
    "step = 3\n",
    "features = []\n",
    "targets = []\n",
    "for i in range(0, len_text - seq_length, step):\n",
    "    features.append(text[i: i + seq_length])\n",
    "    targets.append(text[i + seq_length])\n",
    "num_features = len(features)\n",
    "print('number of features (sequences):'.upper() + str(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((num_features, seq_length, len_chars), dtype=np.bool)\n",
    "y = np.zeros((num_features, len_chars), dtype=np.bool)\n",
    "for i, feature in enumerate(features):\n",
    "    for t, char in enumerate(feature):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[targets[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model Graphs\n",
    "\n",
    "Two RNN graphs are implemented here. One is more deep and less wide and the other is less deep and more wide. The model parameters have been tuned to give decent results. A function is also defined to allow sampling of the train modeled in order to generate new character sequences. In additoin, a function is defined which is called at the end of each epoch to generate and display generated character sequences as the network learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model(seq_length, n_chars):\n",
    "    print('buildinng two-layer model with 128 memory units...'.upper()+'\\n')\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(seq_length, n_chars)))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "    optimizer = RMSprop(lr=0.005)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def wide_model(seq_length, n_chars):\n",
    "    print('building one-layer model with 256 memory units...'.upper()+'\\n')\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(seq_length, n_chars)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "    optimizer = RMSprop(lr=0.005)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def sample(probs, method=0):\n",
    "    probs = np.asarray(probs).astype('float64')\n",
    "    probs = probs/np.sum(probs)\n",
    "    \n",
    "    # helper function to sample an index from a probability array\n",
    "    if method == 0: \n",
    "        # method 0: just return index of character with the highest probability\n",
    "        index = np.argmax(probs)\n",
    "    elif method == 1: \n",
    "        # method 1: draw a random number from 0 to 1, calculate the cumulative sum of the prediction vector.\n",
    "        # return the index of the first element in the cumulative sum greater than the random number\n",
    "        index = np.argwhere(np.cumsum(probs)>np.random.uniform())[0][0]\n",
    "    elif method == 2:\n",
    "        # method 2: draw an element from a multinomial distribution defined by the given probabilities\n",
    "        index = np.argmax(np.random.multinomial(1, probs, 1))\n",
    "    elif method == 3:\n",
    "        # method 3: emphasis larger probabilities and diminish smaller probabilities by doing a log transform\n",
    "        temperature = 0.5 # less than one increases differences between small and large probabilities\n",
    "        probs = np.log(probs) / temperature # same as method 2 with temperature = 1\n",
    "        probs = np.exp(probs) # undo log transform\n",
    "        probs = probs / np.sum(probs)\n",
    "        index = np.argmax(np.random.multinomial(1, probs, 1))\n",
    "        \n",
    "    return index\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    if (epoch)%10 == 0:\n",
    "\n",
    "        print('\\nGENERATING CHARACTER SEQUENCE AFTER EPOCH: {}\\n'.format(epoch+1))\n",
    "\n",
    "        start_index = random.randint(0, len_text - seq_length - 1)\n",
    "       \n",
    "        \n",
    "        seed = text[start_index: start_index + seq_length]\n",
    "        print('GENERATING WITH SEED:\\n\\n' + seed + '\\n')\n",
    "        \n",
    "        \n",
    "        print('MAX SAMPLING:\\n')\n",
    "        phrase = seed\n",
    "        generated = ''\n",
    "        generated += phrase\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(seq_length):\n",
    "            x_pred = np.zeros((1, seq_length, len_chars))\n",
    "            for t, char in enumerate(phrase):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            probs = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(probs, method=0)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            phrase = phrase[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print('\\n')\n",
    "        \n",
    "        print('PROBABILISTIC SAMPLING:\\n')\n",
    "        phrase = seed\n",
    "        generated = ''\n",
    "        generated += phrase\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(seq_length):\n",
    "            x_pred = np.zeros((1, seq_length, len_chars))\n",
    "            for t, char in enumerate(phrase):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            probs = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(probs, method=1)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            phrase = phrase[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDINNG TWO-LAYER MODEL WITH 128 MEMORY UNITS...\n",
      "\n",
      "Epoch 1/41\n",
      "31520/31520 [==============================] - 26s 822us/step - loss: 2.8473 - acc: 0.2083\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 1\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "et this sad interim like the ocean be\n",
      "which parts the shore, where two contracted new\n",
      "come daily to the banks, that when they se\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "et this sad interim like the ocean be\n",
      "which parts the shore, where two contracted new\n",
      "come daily to the banks, that when they ser than sor than sor than sor than sor than sor than sor than sor than sor than sor than sor than sor than sor than sor than sor \n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "et this sad interim like the ocean be\n",
      "which parts the shore, where two contracted new\n",
      "come daily to the banks, that when they seoe, thad, o it woe tout on, oxd ghauu laree\n",
      "fet favee lou si biags anua thoas,\n",
      "coet ny wert poal,\n",
      "huy lose sor youl tin ne fana \n",
      "\n",
      "Epoch 2/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 2.2850 - acc: 0.3367\n",
      "Epoch 3/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 2.0678 - acc: 0.3898\n",
      "Epoch 4/41\n",
      "31520/31520 [==============================] - 25s 784us/step - loss: 1.9408 - acc: 0.4218\n",
      "Epoch 5/41\n",
      "31520/31520 [==============================] - 25s 784us/step - loss: 1.8481 - acc: 0.4454\n",
      "Epoch 6/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 1.7663 - acc: 0.4661\n",
      "Epoch 7/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 1.6966 - acc: 0.4814\n",
      "Epoch 8/41\n",
      "31520/31520 [==============================] - 25s 787us/step - loss: 1.6296 - acc: 0.4982\n",
      "Epoch 9/41\n",
      "31520/31520 [==============================] - 25s 784us/step - loss: 1.5625 - acc: 0.5155\n",
      "Epoch 10/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 1.5010 - acc: 0.5322\n",
      "Epoch 11/41\n",
      "31520/31520 [==============================] - 25s 786us/step - loss: 1.4386 - acc: 0.54552s - loss: 1.4335\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 11\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "n so my sun one early morn did shine,\n",
      "with all triumphant splendour on my brow;\n",
      "but out! alack! he was but one hour mine,\n",
      "the re\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "n so my sun one early morn did shine,\n",
      "with all triumphant splendour on my brow;\n",
      "but out! alack! he was but one hour mine,\n",
      "the reast the world that where the charge the ele,\n",
      "the ence the world that where the chart the world to come part.\n",
      "the le the world th\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "n so my sun one early morn did shine,\n",
      "with all triumphant splendour on my brow;\n",
      "but out! alack! he was but one hour mine,\n",
      "the resper infimber'd that life were thee.\n",
      "xxi\n",
      "i so low'd hast panse;\n",
      "the swect to jest confame mone to loos qatle faisc,\n",
      "to love the \n",
      "\n",
      "Epoch 12/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 1.3733 - acc: 0.5667\n",
      "Epoch 13/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 1.3094 - acc: 0.5860\n",
      "Epoch 14/41\n",
      "31520/31520 [==============================] - 25s 784us/step - loss: 1.2461 - acc: 0.6009\n",
      "Epoch 15/41\n",
      "31520/31520 [==============================] - 25s 786us/step - loss: 1.1800 - acc: 0.6236\n",
      "Epoch 16/41\n",
      "31520/31520 [==============================] - 25s 784us/step - loss: 1.1135 - acc: 0.6419\n",
      "Epoch 17/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 1.0617 - acc: 0.6579\n",
      "Epoch 18/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 1.0040 - acc: 0.6758\n",
      "Epoch 19/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 0.9477 - acc: 0.69557s - ETA: 1s - loss: 0.9434 - a\n",
      "Epoch 20/41\n",
      "31520/31520 [==============================] - 25s 787us/step - loss: 0.9040 - acc: 0.7075\n",
      "Epoch 21/41\n",
      "31520/31520 [==============================] - 25s 786us/step - loss: 0.8539 - acc: 0.72414s - l\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 21\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "ht\n",
      "is more than my o'erpress'd defence can bide?\n",
      "let me excuse thee: ah! my love well knows\n",
      "her pretty looks have been mine enem\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "ht\n",
      "is more than my o'erpress'd defence can bide?\n",
      "let me excuse thee: ah! my love well knows\n",
      "her pretty looks have been mine eneme.\n",
      "xxxiii\n",
      "how what i am those to my must sad me,\n",
      "my selfer to the sweet fabse,\n",
      "when thou strife that were strains be the gaze.\n",
      "c\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "ht\n",
      "is more than my o'erpress'd defence can bide?\n",
      "let me excuse thee: ah! my love well knows\n",
      "her pretty looks have been mine eneme,\n",
      "reserse beaures defect,\n",
      "and the day from the ever not you he,\n",
      "self those thou grownst me tondue,\n",
      "o tho eregrace thy sweet sel\n",
      "\n",
      "Epoch 22/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 0.8145 - acc: 0.7370\n",
      "Epoch 23/41\n",
      "31520/31520 [==============================] - 25s 784us/step - loss: 0.7724 - acc: 0.7490\n",
      "Epoch 24/41\n",
      "31520/31520 [==============================] - 25s 787us/step - loss: 0.7385 - acc: 0.7599\n",
      "Epoch 25/41\n",
      "31520/31520 [==============================] - 25s 786us/step - loss: 0.7064 - acc: 0.7699\n",
      "Epoch 26/41\n",
      "31520/31520 [==============================] - 25s 788us/step - loss: 0.6701 - acc: 0.78093s - loss: \n",
      "Epoch 27/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 0.6428 - acc: 0.7874\n",
      "Epoch 28/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 0.6199 - acc: 0.7970\n",
      "Epoch 29/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 0.5986 - acc: 0.8017\n",
      "Epoch 30/41\n",
      "31520/31520 [==============================] - 25s 787us/step - loss: 0.5748 - acc: 0.8102\n",
      "Epoch 31/41\n",
      "31520/31520 [==============================] - 25s 787us/step - loss: 0.5510 - acc: 0.8155\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 31\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      " the clock for you,\n",
      "nor think the bitterness of absence sour,\n",
      "when you have bid your servant once adieu;\n",
      "nor dare i question wit\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      " the clock for you,\n",
      "nor think the bitterness of absence sour,\n",
      "when you have bid your servant once adieu;\n",
      "nor dare i question with sweet from his guettion did such croan.\n",
      "chill all the reasure, with the sumpents,\n",
      "and they worth thy must truth in my mort,\n",
      "an\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      " the clock for you,\n",
      "nor think the bitterness of absence sour,\n",
      "when you have bid your servant once adieu;\n",
      "nor dare i question with their gaystive-\n",
      "i sell in the freest intime,\n",
      "my alletting wat, winting the time,\n",
      "it no prtayer-soun of hear, and swelied\n",
      "have \n",
      "\n",
      "Epoch 32/41\n",
      "31520/31520 [==============================] - 25s 784us/step - loss: 0.5363 - acc: 0.8227\n",
      "Epoch 33/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 0.5188 - acc: 0.8270\n",
      "Epoch 34/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 0.5076 - acc: 0.8313\n",
      "Epoch 35/41\n",
      "31520/31520 [==============================] - 25s 787us/step - loss: 0.4868 - acc: 0.8386\n",
      "Epoch 36/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 0.4749 - acc: 0.8422\n",
      "Epoch 37/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 0.4645 - acc: 0.84655s -\n",
      "Epoch 38/41\n",
      "31520/31520 [==============================] - 25s 785us/step - loss: 0.4480 - acc: 0.8495\n",
      "Epoch 39/41\n",
      "31520/31520 [==============================] - 25s 786us/step - loss: 0.4413 - acc: 0.8511\n",
      "Epoch 40/41\n",
      "31520/31520 [==============================] - 25s 787us/step - loss: 0.4266 - acc: 0.8568\n",
      "Epoch 41/41\n",
      "31520/31520 [==============================] - 25s 786us/step - loss: 0.4241 - acc: 0.8577\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 41\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "o oft have i invoked thee for my muse,\n",
      "and found such fair assistance in my verse\n",
      "as every alien pen hath got my use\n",
      "and under t\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "o oft have i invoked thee for my muse,\n",
      "and found such fair assistance in my verse\n",
      "as every alien pen hath got my use\n",
      "and under than my wasterming there there;\n",
      "which if he with to be secking with i chest not ther dreath,\n",
      "and they wress bothy now lenes,\n",
      "and \n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "o oft have i invoked thee for my muse,\n",
      "and found such fair assistance in my verse\n",
      "as every alien pen hath got my use\n",
      "and under than my adouth covest take me.\n",
      "cxxii\n",
      "so all i coll love your self if grovt,\n",
      "ind to thy sweet framth but,\n",
      "or land with chyst not b\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f58c87cdfd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deeper, less wide, model\n",
    "model = deep_model(seq_length, len_chars)\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=256,\n",
    "          epochs=41,\n",
    "          # epochs=21, # for frankenstein use fewer epochs because the text is much longer\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING ONE-LAYER MODEL WITH 256 MEMORY UNITS...\n",
      "\n",
      "Epoch 1/41\n",
      "31520/31520 [==============================] - 13s 409us/step - loss: 2.7907 - acc: 0.2268\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 1\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "redeem,\n",
      "in gentle numbers time so idly spent;\n",
      "sing to the ear that doth thy lays esteem\n",
      "and gives thy pen both skill and argumen\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "redeem,\n",
      "in gentle numbers time so idly spent;\n",
      "sing to the ear that doth thy lays esteem\n",
      "and gives thy pen both skill and argumen the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "redeem,\n",
      "in gentle numbers time so idly spent;\n",
      "sing to the ear that doth thy lays esteem\n",
      "and gives thy pen both skill and argumenl'e heouf ans, meve,\n",
      "tee leup beve; ar teve,\n",
      "kn plam -ikeri.nges jethtres,\n",
      "live ly un, whes,,\n",
      "eore.\n",
      "tolljeleee, areid ?fend ouse\n",
      "\n",
      "Epoch 2/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 2.2425 - acc: 0.3508\n",
      "Epoch 3/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 2.0449 - acc: 0.3921\n",
      "Epoch 4/41\n",
      "31520/31520 [==============================] - 13s 398us/step - loss: 1.9192 - acc: 0.42622s -\n",
      "Epoch 5/41\n",
      "31520/31520 [==============================] - 13s 400us/step - loss: 1.8203 - acc: 0.4490\n",
      "Epoch 6/41\n",
      "31520/31520 [==============================] - 12s 396us/step - loss: 1.7348 - acc: 0.4693\n",
      "Epoch 7/41\n",
      "31520/31520 [==============================] - 12s 393us/step - loss: 1.6540 - acc: 0.4934\n",
      "Epoch 8/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 1.6018 - acc: 0.5158\n",
      "Epoch 9/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 1.5062 - acc: 0.5341\n",
      "Epoch 10/41\n",
      "31520/31520 [==============================] - 12s 393us/step - loss: 1.4940 - acc: 0.5546\n",
      "Epoch 11/41\n",
      "31520/31520 [==============================] - 12s 391us/step - loss: 1.8088 - acc: 0.5636\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 11\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "s true?\n",
      "why should he live, now nature bankrupt is,\n",
      "beggar'd of blood to blush through lively veins?\n",
      "for she hath no exchequer n\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "s true?\n",
      "why should he live, now nature bankrupt is,\n",
      "beggar'd of blood to blush through lively veins?\n",
      "for she hath no exchequer now\n",
      "ondest the wild beauty sone in my jeaven;\n",
      "when the with thou dead so for my self delave\n",
      "the with thou dead so for my szeat in\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "s true?\n",
      "why should he live, now nature bankrupt is,\n",
      "beggar'd of blood to blush through lively veins?\n",
      "for she hath no exchequer not inthermonw,\n",
      "then ainht sue; me the myseli than the world acking past,\n",
      "nor that thou outwert toumter absease wild thingzers on\n",
      "\n",
      "Epoch 12/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 1.2439 - acc: 0.6065\n",
      "Epoch 13/41\n",
      "31520/31520 [==============================] - 12s 393us/step - loss: 1.1735 - acc: 0.6304\n",
      "Epoch 14/41\n",
      "31520/31520 [==============================] - 12s 393us/step - loss: 1.0871 - acc: 0.6556\n",
      "Epoch 15/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 1.0160 - acc: 0.6748\n",
      "Epoch 16/41\n",
      "31520/31520 [==============================] - 12s 393us/step - loss: 0.9485 - acc: 0.7000\n",
      "Epoch 17/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 0.8847 - acc: 0.7183\n",
      "Epoch 18/41\n",
      "31520/31520 [==============================] - 12s 391us/step - loss: 0.8337 - acc: 0.7341\n",
      "Epoch 19/41\n",
      "31520/31520 [==============================] - 12s 393us/step - loss: 0.7930 - acc: 0.7494\n",
      "Epoch 20/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 0.7541 - acc: 0.7598\n",
      "Epoch 21/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 0.7251 - acc: 0.7708\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 21\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      "n her head.\n",
      "i have seen roses damask'd, red and white,\n",
      "but no such roses see i in her cheeks;\n",
      "and in some perfumes is there more\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      "n her head.\n",
      "i have seen roses damask'd, red and white,\n",
      "but no such roses see i in her cheeks;\n",
      "and in some perfumes is there more sweet,\n",
      "which is not beauty show thee more sweet,\n",
      "which in is grace in thee i self all your least combors,\n",
      "then i sham staintey \n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      "n her head.\n",
      "i have seen roses damask'd, red and white,\n",
      "but no such roses see i in her cheeks;\n",
      "and in some perfumes is there more sweet,\n",
      "and so porncainsty uthombonce morn that which it muses to remove,\n",
      "oncers beauty cort hast of new\n",
      "wherein in yee, large f\n",
      "\n",
      "Epoch 22/41\n",
      "31520/31520 [==============================] - 12s 394us/step - loss: 0.6885 - acc: 0.7826\n",
      "Epoch 23/41\n",
      "31520/31520 [==============================] - 12s 393us/step - loss: 0.6674 - acc: 0.7883\n",
      "Epoch 24/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 0.6400 - acc: 0.79812s \n",
      "Epoch 25/41\n",
      "31520/31520 [==============================] - 12s 393us/step - loss: 0.6205 - acc: 0.8038\n",
      "Epoch 26/41\n",
      "31520/31520 [==============================] - 12s 393us/step - loss: 0.6064 - acc: 0.8063\n",
      "Epoch 27/41\n",
      "31520/31520 [==============================] - 12s 393us/step - loss: 0.5921 - acc: 0.8109\n",
      "Epoch 28/41\n",
      "31520/31520 [==============================] - 12s 391us/step - loss: 0.5653 - acc: 0.8212\n",
      "Epoch 29/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 0.5587 - acc: 0.8222\n",
      "Epoch 30/41\n",
      "31520/31520 [==============================] - ETA: 0s - loss: 0.5445 - acc: 0.8269- ETA: 2s - l - 12s 392us/step - loss: 0.5444 - acc: 0.8270\n",
      "Epoch 31/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 0.5303 - acc: 0.82960s - loss: 0.5252 - acc: 0\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 31\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      " find room\n",
      "even in the eyes of all posterity\n",
      "that wear this world out to the ending doom.\n",
      "so, till the judgment that yourself ar\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      " find room\n",
      "even in the eyes of all posterity\n",
      "that wear this world out to the ending doom.\n",
      "so, till the judgment that yourself aring,\n",
      "and he worth that they see, they well, to founts, not to heart;\n",
      "and that thou self all ermory dain\n",
      "so provest the submich d\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      " find room\n",
      "even in the eyes of all posterity\n",
      "that wear this world out to the ending doom.\n",
      "so, till the judgment that yourself aring,\n",
      "and deap a doth mpeting shall beant-othersa your lood'st show\n",
      "difdremed beast his gettle fair,\n",
      "to might in than the world b\n",
      "\n",
      "Epoch 32/41\n",
      "31520/31520 [==============================] - 12s 393us/step - loss: 0.5142 - acc: 0.8361\n",
      "Epoch 33/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 0.5017 - acc: 0.8409\n",
      "Epoch 34/41\n",
      "31520/31520 [==============================] - 12s 393us/step - loss: 0.4939 - acc: 0.83942s\n",
      "Epoch 35/41\n",
      "31520/31520 [==============================] - 12s 391us/step - loss: 0.4856 - acc: 0.8443\n",
      "Epoch 36/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 0.4677 - acc: 0.8505\n",
      "Epoch 37/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 0.4579 - acc: 0.8504\n",
      "Epoch 38/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 0.4389 - acc: 0.8546\n",
      "Epoch 39/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 0.4351 - acc: 0.85501s - loss: 0.4322 - \n",
      "Epoch 40/41\n",
      "31520/31520 [==============================] - 12s 392us/step - loss: 0.4210 - acc: 0.85941s - loss: 0.4153\n",
      "Epoch 41/41\n",
      "31520/31520 [==============================] - 12s 393us/step - loss: 0.4166 - acc: 0.8601\n",
      "\n",
      "GENERATING CHARACTER SEQUENCE AFTER EPOCH: 41\n",
      "\n",
      "GENERATING WITH SEED:\n",
      "\n",
      " are dead.\n",
      "cxiii\n",
      "since i left you, mine eye is in my mind;\n",
      "and that which governs me to go about\n",
      "doth part his function and is p\n",
      "\n",
      "MAX SAMPLING:\n",
      "\n",
      " are dead.\n",
      "cxiii\n",
      "since i left you, mine eye is in my mind;\n",
      "and that which governs me to go about\n",
      "doth part his function and is partly brease.\n",
      "cxlii\n",
      "there i love the wart not be thane.\n",
      "cvii\n",
      "that when i shame not the world i do be fare the worsh;\n",
      "and your fa\n",
      "\n",
      "PROBABILISTIC SAMPLING:\n",
      "\n",
      " are dead.\n",
      "cxiii\n",
      "since i left you, mine eye is in my mind;\n",
      "and that which governs me to go about\n",
      "doth part his function and is partly brease;\n",
      "foe thee is the will thee, that so.\n",
      "then i see care, mine eyrory dat,\n",
      "and you heart hath from his find in leaverys\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f58c17e6f90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# less deep but wider model\n",
    "# deeper, less wide, model\n",
    "model = wide_model(seq_length, len_chars)\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=256,\n",
    "          epochs=41,\n",
    "          # epochs=21, # for frankenstein use fewer epochs because the text is much longer\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
